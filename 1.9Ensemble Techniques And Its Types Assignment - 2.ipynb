{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01775dd1-4efa-4add-8fa3-68046cd3721e",
   "metadata": {},
   "source": [
    "### Q1. How does bagging reduce overfitting in decision trees?\n",
    "Bagging reduces overfitting by:\n",
    "- **Training multiple models** on different subsets of the training data (sampled with replacement). Each subset is slightly different, so the model will not overfit to specific quirks of the dataset.\n",
    "- **Averaging the results** of these models (for regression) or using majority voting (for classification). This process smooths out errors, as overfitting tends to be reduced by combining models.\n",
    "\n",
    "### Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "**Advantages:**\n",
    "- **Decision Trees**: Commonly used as base learners because they tend to overfit easily, but bagging controls that overfitting.\n",
    "- **Other learners**: Different types of learners, like SVMs or KNN, can be used to diversify the ensemble, potentially improving performance on specific tasks.\n",
    "  \n",
    "**Disadvantages:**\n",
    "- **Model Complexity**: Using complex learners like SVMs can make the ensemble harder to interpret and increase computational time.\n",
    "- **Computational Cost**: More complex learners require more computational power, making bagging less efficient.\n",
    "\n",
    "### Q3. How does the choice of base learner affect the biasâ€“variance tradeoff in bagging?\n",
    "- **High variance learners** (like decision trees) are ideal for bagging because bagging reduces their variance by averaging predictions over many models.\n",
    "- **Low bias learners** can improve predictive accuracy when combined in a bagging framework, as bagging lowers overfitting.\n",
    "  \n",
    "If the base learner has high bias (e.g., a linear model), bagging is less effective because averaging won't correct systematic errors in the model.\n",
    "\n",
    "### Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "Yes, **bagging** can be used for both classification and regression tasks.\n",
    "- **Classification**: The output from different models is combined using **majority voting**. Each model predicts a class, and the final prediction is the class that gets the most votes.\n",
    "- **Regression**: The predictions from each model are averaged to get the final prediction.\n",
    "\n",
    "### Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "The **ensemble size** refers to the number of models used in bagging. A larger ensemble size generally results in a more stable and accurate prediction. However, after a certain point, increasing the number of models provides diminishing returns in accuracy.\n",
    "\n",
    "- For decision trees, ensembles of **10 to 100 models** often provide good results, but the exact number depends on the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62226f-b2ad-4e73-89af-a593d82656ac",
   "metadata": {},
   "source": [
    "### Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "A **real-world example** of bagging is the **Random Forest** algorithm. Random Forest uses bagging on decision trees to build an ensemble of trees where each tree is trained on a bootstrapped subset of the data. This method is widely used for tasks like:\n",
    "- **Credit scoring**: Predicting the likelihood of a customer defaulting on a loan.\n",
    "- **Medical diagnosis**: Classifying diseases based on patient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35dd500-9b43-461f-80a2-e59e84dda81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest model: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier (which uses bagging)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Random Forest model: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d142da-cd9c-4b0b-88f8-74378f8c9179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
